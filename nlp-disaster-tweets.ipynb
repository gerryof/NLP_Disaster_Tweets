{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":2458964,"sourceType":"datasetVersion","datasetId":1488386},{"sourceId":7903661,"sourceType":"datasetVersion","datasetId":407075}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"pip install gensim","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport spacy\nfrom transformers import BertTokenizer, BertModel \nimport torch \nfrom typing import  Tuple\nfrom sklearn import pipeline, svm\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import  ConfusionMatrixDisplay , precision_score , recall_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nfrom category_encoders.hashing import HashingEncoder\nfrom gensim.test.utils import common_texts","metadata":{"execution":{"iopub.status.busy":"2024-09-27T06:38:43.596433Z","iopub.execute_input":"2024-09-27T06:38:43.596820Z","iopub.status.idle":"2024-09-27T06:39:15.089114Z","shell.execute_reply.started":"2024-09-27T06:38:43.596786Z","shell.execute_reply":"2024-09-27T06:39:15.087931Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Objective\nThe task is to classify Tweets into True/False. True indicates that the tweet refers to an actual natural disaster. False can be anything else. \n\nWe will assume that due to the nature of the task false negative are more problematic that false positive. For this reason we will pay close attention to the recall score.\n\nThis exercise is based on the Kaggle challenge: https://www.kaggle.com/competitions/nlp-getting-started\n\nSources, examples and documentation used by this notebook are referenced at the end. General reference material is used but example of prior completions of this Kaggle excercise are avoided.\n","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain[0:10]","metadata":{"execution":{"iopub.status.busy":"2024-09-27T07:01:59.236337Z","iopub.execute_input":"2024-09-27T07:01:59.239311Z","iopub.status.idle":"2024-09-27T07:01:59.309333Z","shell.execute_reply.started":"2024-09-27T07:01:59.239262Z","shell.execute_reply":"2024-09-27T07:01:59.308223Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n5       1  \n6       1  \n7       1  \n8       1  \n9       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.groupby('target').count()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T06:46:51.427443Z","iopub.execute_input":"2024-05-15T06:46:51.427865Z","iopub.status.idle":"2024-05-15T06:46:51.445423Z","shell.execute_reply.started":"2024-05-15T06:46:51.427830Z","shell.execute_reply":"2024-05-15T06:46:51.444168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is skewed to the negative (4342 vs 3271), which will need to be factored into the evaluation and train test split   \n\nBelow we will look through each of the three features to see if they need to be cleaned up and/or engineered to optimise them for the choosen model.\n\n### The location field:","metadata":{}},{"cell_type":"code","source":"train.groupby(\"location\").id.nunique().sort_values(ascending=True).head(50)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T06:41:16.046596Z","iopub.execute_input":"2024-05-15T06:41:16.047014Z","iopub.status.idle":"2024-05-15T06:41:16.072688Z","shell.execute_reply.started":"2024-05-15T06:41:16.046978Z","shell.execute_reply":"2024-05-15T06:41:16.071733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are some non location strings.\n\nThe locations dont follow a consistant format.\n\nThis feature will need to be cleaned so to ensure there is a consistent naming convention and that each location is a real world location. It may also be usefull to split the parts of the location out so that we test the model with different levels of geographic granularity.","metadata":{}},{"cell_type":"code","source":"train.groupby('target')['location'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T06:44:30.548156Z","iopub.execute_input":"2024-05-15T06:44:30.549170Z","iopub.status.idle":"2024-05-15T06:44:30.559792Z","shell.execute_reply.started":"2024-05-15T06:44:30.549127Z","shell.execute_reply":"2024-05-15T06:44:30.558697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keyword\n","metadata":{}},{"cell_type":"code","source":"[index for index,value in enumerate(no_of_words) if value \n > 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is just one keyword value per field","metadata":{}},{"cell_type":"code","source":"train[(train['keyword'].isna() ==True)].groupby('target').count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As there are only 61 rows missing the keyword , I'll just remove those rows for the training data.","metadata":{}},{"cell_type":"code","source":"train = train['keyword'].dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text\n\nWe can see from the count below that there are no null values for 'text'. These string values contain the twitter hashtags which might be a usefull feature to extract.\n\nIn general the field forms the primary source of information potential in the data set. To maximise this potential this should form the primary area of effort. \n\nIn order to retain the semantic meaning of the sentances we will use a transformer to extract sentance embedings for each tweet. Using the principle of transfer learning we will use a large general purpose transformer like BERT. The embeddings produced will then be combined with the other features and used as the input for the final classification.","metadata":{}},{"cell_type":"code","source":"train[(train['text'].isna() ==True)].groupby('target').count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task Steps","metadata":{}},{"cell_type":"markdown","source":"1. Pipeline choice: identify the best way of tying the different steps together.\n2. Remove non-locations from the location field , ensure a consitent format and split the split the different geographic layers into independant features.\n3. Transform the keword feature into a single word embedding in a usable format as a categorical feature.\n4. Test and select a transformer.\n5. Combine the features\n6. Train and test a classifier.\n7. Make final solution and select model to be submitted.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Location","metadata":{}},{"cell_type":"code","source":"location = train['location'].astype('string')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:10:00.032524Z","iopub.execute_input":"2024-09-26T06:10:00.033055Z","iopub.status.idle":"2024-09-26T06:10:00.040536Z","shell.execute_reply.started":"2024-09-26T06:10:00.033015Z","shell.execute_reply":"2024-09-26T06:10:00.039006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NER LOC & GPE Identification","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")   \n\ndoc_lst = []\n\nfor l in location:\n    if pd.isna(l):\n        doc_lst.append(l)\n    else:\n        doc = nlp(l)\n        doc_lst.append(doc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check results\nfor i in doc_lst[0:100]:\n    if pd.isna(i):\n        'do nothing'\n    else:\n        print([(X.text, X.label_) for X in i.ents])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try with a different model\ntrf = spacy.load(\"en_core_web_lg\") \n\ndoc_lst_trf = []\n\nfor l in location:\n    if pd.isna(l):\n        doc_lst_trf.append(l)\n    else:\n        doc = trf(l)\n        doc_lst_trf.append(doc)\n        \nfor i in doc_lst_trf[0:100]:\n    if pd.isna(i):\n        'do nothing'\n    else:\n        ## print([(X.text, X.label_) for X in i.ents])\n        print(i.text , i. )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alot of locations are being identified as org's. The accuarcy of this method isn't great. Perhaps some rule based matching will work better.","metadata":{}},{"cell_type":"markdown","source":"Calculate the accuracy!","metadata":{}},{"cell_type":"markdown","source":"### Rules based country , city & state extraction","metadata":{}},{"cell_type":"code","source":"cities = pd.read_csv('/kaggle/input/world-cities/worldcities.csv')\ncities.head()                      ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:10:21.463489Z","iopub.execute_input":"2024-09-26T06:10:21.463959Z","iopub.status.idle":"2024-09-26T06:10:21.713293Z","shell.execute_reply.started":"2024-09-26T06:10:21.463918Z","shell.execute_reply":"2024-09-26T06:10:21.712146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def geo_like (source_lst ,geo_lst ):\n    dest_lst = []\n    \n    compiled_regex = [re.compile(r'(?<![^\\W\\d_])' + re.escape(x) + r'(?![^\\W\\d_])', re.IGNORECASE) for x in geo_lst]\n    \n    for i in source_lst:\n        if pd.isna(i):\n            dest_lst.append(None)\n        else:\n            row_gp_lst = [x for x, regex in zip(geo_lst, compiled_regex) if regex.search(i)]\n            if not row_gp_lst :\n                dest_lst.append(None)\n            else:\n                dest_lst.append(row_gp_lst)\n\n    return dest_lst ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:10:26.694873Z","iopub.execute_input":"2024-09-26T06:10:26.695309Z","iopub.status.idle":"2024-09-26T06:10:26.703097Z","shell.execute_reply.started":"2024-09-26T06:10:26.695272Z","shell.execute_reply":"2024-09-26T06:10:26.701703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_long (dest_lst):\n    dest_lst_2 = []\n    for i in dest_lst:\n        if i == None:\n            dest_lst_2.append(None)\n        else:\n            dest_lst_2.append(max(i , key=len))\n    return dest_lst_2","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:10:31.908184Z","iopub.execute_input":"2024-09-26T06:10:31.908577Z","iopub.status.idle":"2024-09-26T06:10:31.914798Z","shell.execute_reply.started":"2024-09-26T06:10:31.908546Z","shell.execute_reply":"2024-09-26T06:10:31.913506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## country\n# the list of countries from the cities dataset doesn't give variations on country names, e.g United States , USA ect. \n# there probably are datasets avaialble that would cover most to of the common purmutations.\n\ncountry_lst = cities['country'].unique()\n\ncountry = find_long(geo_like(location , country_lst))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:10:35.848361Z","iopub.execute_input":"2024-09-26T06:10:35.848806Z","iopub.status.idle":"2024-09-26T06:10:36.657553Z","shell.execute_reply.started":"2024-09-26T06:10:35.848770Z","shell.execute_reply":"2024-09-26T06:10:36.656232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##  city\n\ncity_lst = cities['city'].unique()\ncity = find_long(geo_like(location , city_lst))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:10:39.228259Z","iopub.execute_input":"2024-09-26T06:10:39.229443Z","iopub.status.idle":"2024-09-26T06:13:09.256732Z","shell.execute_reply.started":"2024-09-26T06:10:39.229397Z","shell.execute_reply":"2024-09-26T06:13:09.254982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## state \n\nstates = pd.read_csv('/kaggle/input/startup-success-prediction-dataset/D3/states.csv')\n\nstates_name_lst = states['State'].unique()\n\nstates_abv_lst = states['Abbreviation'].unique()\n\nstate_name = find_long(geo_like(location , states_name_lst))\n\nstate_abv = find_long(geo_like(location , states_abv_lst ))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:15:50.829603Z","iopub.execute_input":"2024-09-26T06:15:50.831499Z","iopub.status.idle":"2024-09-26T06:15:51.346921Z","shell.execute_reply.started":"2024-09-26T06:15:50.831437Z","shell.execute_reply":"2024-09-26T06:15:51.345588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"states.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## add to test dataset\n\ntrain['country'] = country\ntrain['city'] = city\ntrain['state'] = state_name\ntrain['state_abv'] = state_abv","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:15:56.126820Z","iopub.execute_input":"2024-09-26T06:15:56.127262Z","iopub.status.idle":"2024-09-26T06:15:56.140300Z","shell.execute_reply.started":"2024-09-26T06:15:56.127225Z","shell.execute_reply":"2024-09-26T06:15:56.138694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[(train['location'].isna() ==False)].head()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T07:39:23.014829Z","iopub.execute_input":"2024-05-15T07:39:23.015965Z","iopub.status.idle":"2024-05-15T07:39:23.032137Z","shell.execute_reply.started":"2024-05-15T07:39:23.015922Z","shell.execute_reply":"2024-05-15T07:39:23.031078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in blank countries where the city has been identified\nsingilton = cities.groupby('city')[\"country\"].nunique().loc[lambda x: x==1].sort_values()\n\ncity_country = cities.merge(singilton , how = 'inner' , left_on ='city' , right_on = 'city')[[\"city\" , \"country_x\"]].drop_duplicates()\n\ntrain = train.merge(city_country , how ='left' , left_on = 'city', right_on = 'city'  )\n\ntrain['country'] = train['country'].fillna(train['country_x'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:16:08.163862Z","iopub.execute_input":"2024-09-26T06:16:08.164336Z","iopub.status.idle":"2024-09-26T06:16:08.381954Z","shell.execute_reply.started":"2024-09-26T06:16:08.164296Z","shell.execute_reply":"2024-09-26T06:16:08.380254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in blank countries where the state has been identified \ntrain['country'] = train[(train['state'].isna() == False) | (train['state_abv'].isna() == False)]['country'].fillna(\"United States\")\n\n# create one state column with the two letter code","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:16:16.429107Z","iopub.execute_input":"2024-09-26T06:16:16.430236Z","iopub.status.idle":"2024-09-26T06:16:16.443773Z","shell.execute_reply.started":"2024-09-26T06:16:16.430192Z","shell.execute_reply":"2024-09-26T06:16:16.441971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[( train['location'].isna() == False)].head()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T07:40:11.469742Z","iopub.execute_input":"2024-05-15T07:40:11.470124Z","iopub.status.idle":"2024-05-15T07:40:11.488222Z","shell.execute_reply.started":"2024-05-15T07:40:11.470092Z","shell.execute_reply":"2024-05-15T07:40:11.487132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[(train['city'].isna() ==True)].groupby('target').count()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T07:44:59.765669Z","iopub.execute_input":"2024-05-15T07:44:59.766057Z","iopub.status.idle":"2024-05-15T07:44:59.784556Z","shell.execute_reply.started":"2024-05-15T07:44:59.766023Z","shell.execute_reply":"2024-05-15T07:44:59.783575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# what to do about the null locations?","metadata":{}},{"cell_type":"markdown","source":"# Pipeline pre-processing steps","metadata":{}},{"cell_type":"markdown","source":"## Keyword preprocessing","metadata":{}},{"cell_type":"markdown","source":"Need to find an alternitive to gensim's word2vec, it's sklearn api is unsupported. \nShould be possible to find a sklearn or scipy text vectoriser for optimized for a single word that still includes a semantic understanding in terms of its location in the vector space.","metadata":{}},{"cell_type":"code","source":"class WordVectorTransformer(TransformerMixin,BaseEstimator):\n    def __init__(self, model=\"en_core_web_lg\"):\n        self.model = model\n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        nlp = spacy.load(self.model)\n        return np.concatenate([nlp(word).vector.reshape(1,-1) for word in X])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handeling the skew","metadata":{}},{"cell_type":"markdown","source":"this is handeled in the svm linear estimator by the balance parameter, perhaps something similar can be done for the other estimaters?","metadata":{}},{"cell_type":"markdown","source":"## Text preprocessing","metadata":{}},{"cell_type":"code","source":"class tokenizer( BaseEstimator,TransformerMixin):\n    def __init__(\n        self, variables\n    ):\n        self.pre_trained = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.add_special_tokens = True\n        self.var = variables\n        \n    def _tokenize(self, text :str) :\n        tokenized = self.pre_trained.encode_plus(\n            text,\n            add_special_tokens = self.add_special_tokens,\n            max_length = 512, \n            )\n        return (\n            torch.tensor(tokenized[\"input_ids\"]).unsqueeze(0),\n            torch.tensor(tokenized[\"attention_mask\"]).unsqueeze(0),\n        )\n    \n    def transform ( self, X):\n        col = self.var\n        text = X['col'].tolist()\n        with torch.no_grad():\n            X['col'] = [self._tokenize(string) for string in text]\n            #step1_out = step1_out.values\n            return X\n\n    def fit( self, X, y=None):\n        return self","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:31:45.613236Z","iopub.execute_input":"2024-04-01T09:31:45.613594Z","iopub.status.idle":"2024-04-01T09:31:45.623243Z","shell.execute_reply.started":"2024-04-01T09:31:45.613565Z","shell.execute_reply":"2024-04-01T09:31:45.622164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class bertmodel(BaseEstimator,TransformerMixin):\n    def __init__(\n        self , variables\n    ):\n        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.var = variables\n    \n    def _berty (self , tolkens , attention_mask):\n        with torch.no_grad():\n          embeddings = self.bert_model(tolkens, attention_mask = attention_mask)\n        last_hidden_state = embeddings[0]\n        get_cls = last_hidden_state[:, 0, :]\n        \n        return get_cls \n    def transform ( self, X):\n        col = self.var\n        tolkenized_text = x[] # how to ensure that each step gets the variables it needs, works on the variables its supposed to and that the final output contains all of the the pre-processed features and not the others/ or maybe i dont have to remove the others??\n        with torch.no_grad():\n            return torch.stack([self._berty(tolkens , attention_mask) for tolkens , attention_mask in X])[:, 0, :]\n\n    def fit(self, X, y=None):\n        return self\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:32:36.507198Z","iopub.execute_input":"2024-04-01T09:32:36.507687Z","iopub.status.idle":"2024-04-01T09:32:36.517370Z","shell.execute_reply.started":"2024-04-01T09:32:36.507650Z","shell.execute_reply":"2024-04-01T09:32:36.516124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Location preprocessing","metadata":{}},{"cell_type":"code","source":"class hashingcustom(BaseEstimator, TransformerMixin):\n    def __init__(self, variables):\n        self.variables = variables\n        self.he = HashingEncoder(\n            cols = variables, \n            n_components=20*len(variables)\n    def fit(self, X, y = None): #may need to try with y_train values, couldn't find explenation why the hashing encoder would need this data but the example & documentation seems to suggest it does.\n        X_ = X.loc[:,self.variables]\n        self.he.fit(X_)\n        return self\n    def transform(self, X):\n        X_ = X.loc[:,self.variables]\n        X_transformed =   \n            pd.DataFrame(self.he.transform(X_).toarray(), \n            columns= self.he.get_feature_names_out())\n        X.drop(self.variables, axis= 1, inplace=True)\n        X[self.he.get_feature_names_out()] = \n            X_transformed[self.he.get_feature_names_out()].values\n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Pipeline GridSearch , Cross Validation and Scoring","metadata":{}},{"cell_type":"code","source":"# https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\n\ndef print_dataframe(filtered_cv_results):\n    \"\"\"Pretty print for filtered dataframe\"\"\"\n    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n        filtered_cv_results[\"mean_test_precision\"],\n        filtered_cv_results[\"std_test_precision\"],\n        filtered_cv_results[\"mean_test_recall\"],\n        filtered_cv_results[\"std_test_recall\"],\n        filtered_cv_results[\"params\"],\n    ):\n        print(\n            f\"precision: {mean_precision:0.3f} (±{std_precision:0.03f}),\"\n            f\" recall: {mean_recall:0.3f} (±{std_recall:0.03f}),\"\n            f\" for {params}\"\n        )\n    print()\n\n\ndef refit_strategy(cv_results):\n    # print the info about the grid-search for the different scores\n    precision_threshold = 0.75\n\n    cv_results_ = pd.DataFrame(cv_results)\n    print(\"All grid-search results:\")\n    print_dataframe(cv_results_)\n\n    # Filter-out all results below the threshold\n    high_precision_cv_results = cv_results_[\n        cv_results_[\"mean_test_precision\"] > precision_threshold\n    ]\n\n    print(f\"Models with a precision higher than {precision_threshold}:\")\n    print_dataframe(high_precision_cv_results)\n\n    high_precision_cv_results = high_precision_cv_results[\n        [\n            \"mean_score_time\",\n            \"mean_test_recall\",\n            \"mean_test_precision\",\n            \"std_test_recall\",\n            \"std_test_precision\",\n            \"rank_test_recall\",\n            \"rank_test_precision\",\n            \"params\",\n        ]\n    ]\n\n    # Select the most performant models in terms of recall\n    # (within 1 sigma from the best)\n    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n    best_recall_threshold = best_recall - best_recall_std\n\n    high_recall_cv_results = high_precision_cv_results[\n        high_precision_cv_results[\"mean_test_recall\"] > best_recall_threshold\n    ]\n    print(\n        \"Out of the previously selected high precision models, we keep all the\\n\"\n        \"the models within one standard deviation of the highest recall model:\"\n    )\n    print_dataframe(high_recall_cv_results)\n    \"\"\"\n    # From the best candidates, select the fastest model to predict\n    fastest_top_recall_high_precision_index = high_recall_cv_results[\n        \"mean_score_time\"\n    ].idxmin()\n\n    print(\n        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n        \"selected subset of best models based on precision and recall.\\n\"\n        \"Its scoring time is:\\n\\n\"\n        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n    )\n    \n    return fastest_top_recall_high_precision_index\n    \"\"\"\n    return high_recall_cv_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay.from_predictions(test_y, y_pred)\n#disp.plot()\nplt.show()\nprint('Precision: %.3f' % precision_score(test_y, y_pred))\nprint('Recall: %.3f' % recall_score(test_y, y_pred))\nprint('F1: %.3f' % f1_score(test_y, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(test_y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-03-10T09:56:48.647572Z","iopub.execute_input":"2024-03-10T09:56:48.648268Z","iopub.status.idle":"2024-03-10T09:56:49.172180Z","shell.execute_reply.started":"2024-03-10T09:56:48.648213Z","shell.execute_reply":"2024-03-10T09:56:49.170581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Pipeline & Find the Best Model & Parameters","metadata":{}},{"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(train['text'], train['target'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:39:59.512571Z","iopub.execute_input":"2024-04-01T09:39:59.513888Z","iopub.status.idle":"2024-04-01T09:39:59.529465Z","shell.execute_reply.started":"2024-04-01T09:39:59.513836Z","shell.execute_reply":"2024-04-01T09:39:59.528280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"questions left:\nIf i just use the cls token does that capture multi sentenance tweets correctly?/\n\nhow does the sklearn pipeline know what to pass as an output from one step to the inputs of the next step/\n\ncan i use udf's instead of class's for the pipeline steps?/\nhow do you navigate through a tensors structure / how does a tensor work?/\nshould i be using the attention mask or is it being used by default?\n\nDo i need to pre initialise the estimators or do it in the fit method of each step?\n\nShould I be using the fit or fit_transpform methods of the pipeline?\n\nThe classes work individually and together, outside the pipeline. It's the bastard pipeline thats' making stringing the stes together difficult. Perhaps this does suggest something to do with the initialisation.\n\nhttps://medium.com/@benlc77/how-to-write-clean-and-scalable-code-with-custom-transformers-sklearn-pipelines-ecb8e53fe110\n","metadata":{}},{"cell_type":"code","source":"# configure the steps of the pre-processing sub-pipes\ntext_pre_process_pipe = pipeline.Pipeline(\n    steps=[\n        ('Tokenize' , tokenizer),\n        ('embed' , bertmodel)\n    ]\n)\n\nkeyword_pre_process_pipe = pipeline.Pipeline(\n    steps[\n        ('word2vec' , wordvec)\n    ])\n\nloc_pre_processing = pipeline.Pipeline(\n    steps[\n        ('hash' , hashing)\n    ])\n\ncombined_preprocessing = pipeline.FeatureUnion([\n    ('text', text_pre_process_pipe),\n    ('keyword', keyword_pre_process_pipe),\n    ('geo', loc_pre_processing),\n])\n# need to think about surfacing the parameters up as inputs into the class for grid search parameter tuning","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set up the gridsearchcv parameter grid and selection of models to test\nsvm = svm.SVM()\n\nscores = [\"precision\" , \"recall\"]\n\nparam_svm = [\n    {\"kernel\" : [\"linear\"], \"C\": [1, 10, 100, 1000], \"multi_class\" :[\"ovr\", \"crammer_singer\"],\"class_weightdict\" : [\"balanced\"]},\n    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n]\n\nparam_nb = []\n\nparam_gbt = []\n\nparam_rf = []\n\nclass_models = [ #should this be a dict of lists e.g {\"model\": [svm , nd , gbt , rf] , \"param\"}\n    {\"name\" : \"svm\", \"model\": svm, \"param\": param_svm},\n    # {\"name\" : \"Naive Bayes\", \"model\": nb , \"param\" : param_nb},\n    #{\"name\": \"gradient boosted trees\" ,\"model\" : gbt , \"param\": param_gbt},\n    #{\"name\": \"random forest\", \"model\": rf , \"param\": param_rf}\n    #https://towardsdatascience.com/naive-bayes-classifier-explained-50f9723571ed,\n    # gradient boosted trees\n    # random forest\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-31T05:22:06.596880Z","iopub.execute_input":"2024-07-31T05:22:06.597340Z","iopub.status.idle":"2024-07-31T05:22:06.604967Z","shell.execute_reply.started":"2024-07-31T05:22:06.597301Z","shell.execute_reply":"2024-07-31T05:22:06.603159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialise the classes\nbertmodel = bertmodel(variables = ['text'])\ntokenizer = tokenizer(variables = ['text'])\nhashing = hashingcustom(variables = ['country' , 'city' , 'state'] )\nwordvec = WordVectorTransformer(variables = 'keyword' )","metadata":{"execution":{"iopub.status.busy":"2024-04-01T09:40:04.809695Z","iopub.execute_input":"2024-04-01T09:40:04.810189Z","iopub.status.idle":"2024-04-01T09:40:09.115615Z","shell.execute_reply.started":"2024-04-01T09:40:04.810153Z","shell.execute_reply":"2024-04-01T09:40:09.114487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#iterate through the model's being tested and hyperparamter tuning\nfor name, model, param in class_models.items():\n  \n    complete_pipeline = Pipeline([\n        ('preprocessing', combined_preprocessing),\n        ('Model Training', GridSearchCV(estimator=model,param_grid=param, scoring=scores , n_jobs=2,refit=refit_strategy, c=6 )\n    ])\n    \n    # model fitting\n    complete_pipeline.fit(train_x, train_y)\n    \n    # model scoring\n    test_pred = complete_pipeline.predict(test_x)\n    \n        # review this given the refit strategy\n    # Evaluate model performance\n    disp = ConfusionMatrixDisplay.from_predictions(test_y, test_pred)\n    plt.show()\n    print('Precision: %.3f' % precision_score(test_y, y_pred))\n    print('Recall: %.3f' % recall_score(test_y, y_pred))\n    print('F1: %.3f' % f1_score(test_y, y_pred))\n    print('Accuracy: %.3f' % accuracy_score(test_y, y_pred))\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"parameter tuning:\nhttps://scikit-learn.org/stable/modules/grid_search.html\n\nmodel selection\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html","metadata":{}},{"cell_type":"markdown","source":"## TwHIN-BERT","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('Twitter/twhin-bert-base')\nmodel = AutoModel.from_pretrained('Twitter/twhin-bert-base')\ninputs = tokenizer(\"I'm using TwHIN-BERT! #TwHIN-BERT #NLP\", return_tensors=\"pt\")\noutputs = model(**inputs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection & Final Submission","metadata":{}},{"cell_type":"markdown","source":"### References\n\nhttps://towardsdatascience.com/build-a-bert-sci-kit-transformer-59d60ddd54a5\n\nhttps://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b\n\nhttps://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\nhttps://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n\n@article{zhang2022twhin,\n  title={TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations},\n  author={Zhang, Xinyang and Malkov, Yury and Florez, Omar and Park, Serim and McWilliams, Brian and Han, Jiawei and El-Kishky, Ahmed},\n  journal={arXiv preprint arXiv:2209.07562},\n  year={2022}\n}\n\nhttps://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e\n\ntokenization:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils.py\nhttps://github.com/google-research/bert/blob/master/tokenization.py\nnone the wiser on how the special tokens handels #, im guessing it doesn't extract the semantic meaning.\n\nhttps://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\nhttps://datasciencetoday.net/index.php/en-us/nlp/211-paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained\n\nhttps://lifewithdata.com/2023/05/27/transformermixin-in-scikit-learn/\n\nhttps://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13#b13b\n\nhttps://stackoverflow.com/questions/43366561/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once\n\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\n\nhttps://lvngd.com/blog/spacy-word-vectors-as-features-in-scikit-learn/\nStill not sure how flattening the vector array doesn't balloon out the number of features the model has to handel what issues this might cause.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV","metadata":{},"execution_count":null,"outputs":[]}]}