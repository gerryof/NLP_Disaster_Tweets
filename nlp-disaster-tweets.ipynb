{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":2458964,"sourceType":"datasetVersion","datasetId":1488386},{"sourceId":5294033,"sourceType":"datasetVersion","datasetId":407075}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\n#import spacy\nfrom transformers import BertTokenizer, BertModel \nimport torch \nfrom typing import  Tuple\nfrom sklearn import pipeline, svm\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import  ConfusionMatrixDisplay , precision_score , recall_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-03-11T06:55:09.536841Z","iopub.execute_input":"2024-03-11T06:55:09.537293Z","iopub.status.idle":"2024-03-11T06:55:19.154223Z","shell.execute_reply.started":"2024-03-11T06:55:09.537253Z","shell.execute_reply":"2024-03-11T06:55:19.152945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# '/Users/geraldoflanagain/Downloads'\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-03-10T07:15:04.378127Z","iopub.execute_input":"2024-03-10T07:15:04.378557Z","iopub.status.idle":"2024-03-10T07:15:04.392633Z","shell.execute_reply.started":"2024-03-10T07:15:04.378522Z","shell.execute_reply":"2024-03-10T07:15:04.391467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n#train = pd.read_csv('/Users/geraldoflanagain/Downloads/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T06:55:27.849829Z","iopub.execute_input":"2024-03-11T06:55:27.850613Z","iopub.status.idle":"2024-03-11T06:55:27.908610Z","shell.execute_reply.started":"2024-03-11T06:55:27.850570Z","shell.execute_reply":"2024-03-11T06:55:27.907246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[0:10]","metadata":{"execution":{"iopub.status.busy":"2024-02-05T11:11:35.957804Z","iopub.execute_input":"2024-02-05T11:11:35.958270Z","iopub.status.idle":"2024-02-05T11:11:35.983265Z","shell.execute_reply.started":"2024-02-05T11:11:35.958236Z","shell.execute_reply":"2024-02-05T11:11:35.981927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What does the location field look like and does it need cleaning?","metadata":{}},{"cell_type":"code","source":"train.groupby(\"location\").id.nunique().sort_values(ascending=True).head(50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are some non location strings.","metadata":{}},{"cell_type":"code","source":"train.groupby('target')['location'].nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initial Steps:\n\n1/ use spacy to apply NER to the location field and remove non-locations. \n\n2/ second step of EDA on the key word field but the initial thoughts would be to train a key word extraction model on the records with keywords and apply it to the blanks.\n\n3/ use scikit learn or spacy  to create a pipeline with feature union to concat the tags, location and text vectors together. \n\n4/ pick a transformer - prefairably a pre-trained one. \n\n5/ Output layer with the concatonated vectors and the datasets labels.\n","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Location","metadata":{}},{"cell_type":"code","source":"location = train['location'].astype('string')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NER LOC & GPE Identification","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")   \n\ndoc_lst = []\n\nfor l in location:\n    if pd.isna(l):\n        doc_lst.append(l)\n    else:\n        doc = nlp(l)\n        doc_lst.append(doc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check results\nfor i in doc_lst[0:100]:\n    if pd.isna(i):\n        'do nothing'\n    else:\n        print([(X.text, X.label_) for X in i.ents])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try with a different model\ntrf = spacy.load(\"en_core_web_lg\") \n\ndoc_lst_trf = []\n\nfor l in location:\n    if pd.isna(l):\n        doc_lst_trf.append(l)\n    else:\n        doc = trf(l)\n        doc_lst_trf.append(doc)\n        \nfor i in doc_lst_trf[0:100]:\n    if pd.isna(i):\n        'do nothing'\n    else:\n        ## print([(X.text, X.label_) for X in i.ents])\n        print(i.text , i. )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alot of locations are being identified as org's. From spot checking the accuracy of this method doesn't seem great. Perhaps some rule based matching.","metadata":{}},{"cell_type":"markdown","source":"### Rules based country , city & state extraction","metadata":{}},{"cell_type":"code","source":"cities = pd.read_csv('/Users/geraldoflanagain/Downloads/worldcities.csv')\ncities.head()                      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def geo_like (source_lst ,geo_lst ):\n    dest_lst = []\n    \n    compiled_regex = [re.compile(r'(?<![^\\W\\d_])' + re.escape(x) + r'(?![^\\W\\d_])', re.IGNORECASE) for x in geo_lst]\n    \n    for i in source_lst:\n        if pd.isna(i):\n            dest_lst.append(None)\n        else:\n            row_gp_lst = [x for x, regex in zip(geo_lst, compiled_regex) if regex.search(i)]\n            if not row_gp_lst :\n                dest_lst.append(None)\n            else:\n                dest_lst.append(row_gp_lst)\n\n    return dest_lst ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_long (dest_lst):\n    dest_lst_2 = []\n    for i in dest_lst:\n        if i == None:\n            dest_lst_2.append(None)\n        else:\n            dest_lst_2.append(max(i , key=len))\n    return dest_lst_2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## country\n# the list of countries from the cities dataset doesn't give variations on country names, e.g United States , USA ect. \n# there probably are datasets avaialble that would cover most to of the common purmutations.\n\ncountry_lst = cities['country'].unique()\n\ncountry = find_long(geo_like(location , country_lst))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##  city\n\ncity_lst = cities['city'].unique()\ncity = find_long(geo_like(location , city_lst))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## state \n\nstates = pd.read_csv('/Users/geraldoflanagain/Downloads/states.csv')\n\nstates_name_lst = states['State'].unique()\n\nstates_abv_lst = states['Abbreviation'].unique()\n\nstate_name = find_long(geo_like(location , states_name_lst))\n\nstate_abv = find_long(geo_like(location , states_abv_lst ))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"states.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## add to test dataset\n\ntrain['country'] = country\ntrain['city'] = city\ntrain['state'] = state_name\ntrain['state_abv'] = state_abv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[(train['location'].isna() ==False)].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in blank countries where the city has been identified\nsingilton = cities.groupby('city')[\"country\"].nunique().loc[lambda x: x==1].sort_values()\n\ncity_country = cities.merge(singilton , how = 'inner' , left_on ='city' , right_on = 'city')[[\"city\" , \"country_x\"]].drop_duplicates()\n\ntrain = train.merge(city_country , how ='left' , left_on = 'city', right_on = 'city'  )\n\ntrain['country'] = train['country'].fillna(train['country_x'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in blank countries where the state has been identified \ntrain['country'] = train[(train['state'].isna() == False) | (train['state_abv'].isna() == False)]['country'].fillna(\"United States\")\n\n# create one state column with the two letter code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[( train['location'].isna() == False)].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Key word extraction\nUsing the values already populated  in the keyword column, train a model to extract keywords for the null values.","metadata":{}},{"cell_type":"code","source":"[index for index,value in enumerate(no_of_words) if value \n > 1]","metadata":{"execution":{"iopub.status.busy":"2024-03-11T07:14:13.304769Z","iopub.execute_input":"2024-03-11T07:14:13.305277Z","iopub.status.idle":"2024-03-11T07:14:13.315268Z","shell.execute_reply.started":"2024-03-11T07:14:13.305240Z","shell.execute_reply":"2024-03-11T07:14:13.313552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is just one keyword value per row.","metadata":{}},{"cell_type":"code","source":"train[(train['keyword'].isna() ==True)].groupby('target').count()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As its just 61 rows i will remove them from the dataset.","metadata":{}},{"cell_type":"code","source":"train = train['keyword'].dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"its not clear which is better to use, something like word2vec which gives a single vector for each word or to use a transformer model like bert. The problem there is clearly that bert splits the  words down into sub tokens, which doesn't seem like the best information extraction for a single word feature.","metadata":{}},{"cell_type":"code","source":"from gensim.test.utils import common_texts\nfrom gensim.sklearn_api import W2VTransformer\n\n# Create a model to represent each word by a 10 dimensional vector.\nmodel = W2VTransformer(size=10, min_count=1, seed=1)\n\n# What is the vector representation of the word 'graph'?\nwordvecs = model.fit(common_texts).transform(['graph', 'system'])\n\n# I think i need a customer transformer in order to be able to aply this ti just one feature\n# https://www.kaggle.com/code/tarekyahia/word2vec-custom-column-transformer-pipelines\nimport gensim\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,r2_score\n\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nclass preprocess_s2v(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        self.model = gensim.models.Word2Vec(X,\n                                   vector_size=150,\n                                   window=5,\n                                   min_count=1)\n        self.words =  set(self.model.wv.index_to_key)\n        return self\n    def transform(self, X):\n        X_vecs = np.array([np.array([self.model.wv[i] for i in ls if i in self.words]) for ls in X], dtype=object)\n        X = np.array([vs.mean(axis = 0) if vs.size else np.zeros(100, dtype=float) for vs in X_vecs])\n        return np.array(X)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T07:47:30.772605Z","iopub.execute_input":"2024-03-11T07:47:30.773517Z","iopub.status.idle":"2024-03-11T07:47:58.278973Z","shell.execute_reply.started":"2024-03-11T07:47:30.773463Z","shell.execute_reply":"2024-03-11T07:47:58.277387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Classifier","metadata":{}},{"cell_type":"markdown","source":"## bert-base-uncased","metadata":{}},{"cell_type":"code","source":"class tokenizer( BaseEstimator,TransformerMixin):\n    def __init__(\n        self,\n    ):\n        self.pre_trained = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.add_special_tokens = True\n        \n    def _tokenize(self, text :str) :\n        tokenized = self.pre_trained.encode_plus(\n            text,\n            add_special_tokens = self.add_special_tokens,\n            max_length = 512, \n            )\n        return (\n            torch.tensor(tokenized[\"input_ids\"]).unsqueeze(0),\n            torch.tensor(tokenized[\"attention_mask\"]).unsqueeze(0),\n        )\n    \n    def transform ( self, X):\n        text = X.tolist()\n        with torch.no_grad():\n            X = [self._tokenize(string) for string in text]\n            #step1_out = step1_out.values\n            return X\n\n    def fit( self, X, y=None):\n        return self","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:31:12.536404Z","iopub.execute_input":"2024-03-10T07:31:12.537781Z","iopub.status.idle":"2024-03-10T07:31:12.549256Z","shell.execute_reply.started":"2024-03-10T07:31:12.537734Z","shell.execute_reply":"2024-03-10T07:31:12.547203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class bertmodel(BaseEstimator,TransformerMixin):\n    def __init__(\n        self\n    ):\n        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n    \n    def _berty (self , tolkens , attention_mask):\n        with torch.no_grad():\n          embeddings = self.bert_model(tolkens, attention_mask = attention_mask)\n        last_hidden_state = embeddings[0]\n        get_cls = last_hidden_state[:, 0, :]\n        \n        return get_cls \n    def transform ( self, X):\n        with torch.no_grad():\n            return torch.stack([self._berty(tolkens , attention_mask) for tolkens , attention_mask in X])[:, 0, :]\n\n    def fit(self, X, y=None):\n        return self\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:31:16.691585Z","iopub.execute_input":"2024-03-10T07:31:16.692000Z","iopub.status.idle":"2024-03-10T07:31:16.702710Z","shell.execute_reply.started":"2024-03-10T07:31:16.691968Z","shell.execute_reply":"2024-03-10T07:31:16.700764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(train['text'], train['target'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:31:43.988187Z","iopub.execute_input":"2024-03-10T07:31:43.988568Z","iopub.status.idle":"2024-03-10T07:31:44.004509Z","shell.execute_reply.started":"2024-03-10T07:31:43.988538Z","shell.execute_reply":"2024-03-10T07:31:44.003179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"questions left:\nIf i just use the cls token does that capture multi sentenance tweets correctly?/\n\nhow does the sklearn pipeline know what to pass as an output from one step to the inputs of the next step/\n\ncan i use udf's instead of class's for the pipeline steps?/\nhow do you navigate through a tensors structure / how does a tensor work?/\nshould i be using the attention mask or is it being used by default?\n\nDo i need to pre initialise the estimators or do it in the fit method of each step?\n\nShould I be using the fit or fit_transpform methods of the pipeline?\n\nThe classes work individually and together, outside the pipeline. It's the bastard pipeline thats' making stringing the stes together difficult. Perhaps this does suggest something to do with the initialisation.\n\nhttps://medium.com/@benlc77/how-to-write-clean-and-scalable-code-with-custom-transformers-sklearn-pipelines-ecb8e53fe110\n","metadata":{}},{"cell_type":"code","source":"classifier = svm.LinearSVC(C=1.0, class_weight=\"balanced\")\nbertmodel = bertmodel()\ntokenizer = tokenizer()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:31:50.472695Z","iopub.execute_input":"2024-03-10T07:31:50.473133Z","iopub.status.idle":"2024-03-10T07:31:56.414517Z","shell.execute_reply.started":"2024-03-10T07:31:50.473097Z","shell.execute_reply":"2024-03-10T07:31:56.413269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = pipeline.Pipeline(\n    [\n        (\"tokenizer\",  tokenizer ),\n        (\"vectorizer\", bertmodel ),\n        (\"classifier\", classifier),\n    ]\n)\nmodel.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:32:02.092877Z","iopub.execute_input":"2024-03-10T07:32:02.093292Z","iopub.status.idle":"2024-03-10T07:41:57.579617Z","shell.execute_reply.started":"2024-03-10T07:32:02.093258Z","shell.execute_reply":"2024-03-10T07:41:57.578465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:18:08.435281Z","iopub.execute_input":"2024-03-10T07:18:08.435785Z","iopub.status.idle":"2024-03-10T07:18:08.463512Z","shell.execute_reply.started":"2024-03-10T07:18:08.435751Z","shell.execute_reply":"2024-03-10T07:18:08.462442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:02:24.981327Z","iopub.execute_input":"2024-03-10T08:02:24.981833Z","iopub.status.idle":"2024-03-10T08:02:24.988044Z","shell.execute_reply.started":"2024-03-10T08:02:24.981773Z","shell.execute_reply":"2024-03-10T08:02:24.986898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-10T08:05:46.833158Z","iopub.execute_input":"2024-03-10T08:05:46.833617Z","iopub.status.idle":"2024-03-10T08:05:46.838565Z","shell.execute_reply.started":"2024-03-10T08:05:46.833585Z","shell.execute_reply":"2024-03-10T08:05:46.837354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_x)  ","metadata":{"execution":{"iopub.status.busy":"2024-03-10T07:45:12.641437Z","iopub.execute_input":"2024-03-10T07:45:12.641838Z","iopub.status.idle":"2024-03-10T07:47:41.080869Z","shell.execute_reply.started":"2024-03-10T07:45:12.641808Z","shell.execute_reply":"2024-03-10T07:47:41.076754Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay.from_predictions(test_y, y_pred)\n#disp.plot()\nplt.show()\nprint('Precision: %.3f' % precision_score(test_y, y_pred))\nprint('Recall: %.3f' % recall_score(test_y, y_pred))\nprint('F1: %.3f' % f1_score(test_y, y_pred))\nprint('Accuracy: %.3f' % accuracy_score(test_y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-03-10T09:56:48.647572Z","iopub.execute_input":"2024-03-10T09:56:48.648268Z","iopub.status.idle":"2024-03-10T09:56:49.172180Z","shell.execute_reply.started":"2024-03-10T09:56:48.648213Z","shell.execute_reply":"2024-03-10T09:56:49.170581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TwHIN-BERT","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('Twitter/twhin-bert-base')\nmodel = AutoModel.from_pretrained('Twitter/twhin-bert-base')\ninputs = tokenizer(\"I'm using TwHIN-BERT! #TwHIN-BERT #NLP\", return_tensors=\"pt\")\noutputs = model(**inputs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References\n\nhttps://towardsdatascience.com/build-a-bert-sci-kit-transformer-59d60ddd54a5\n\nhttps://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b\n\nhttps://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\nhttps://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n\n@article{zhang2022twhin,\n  title={TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations},\n  author={Zhang, Xinyang and Malkov, Yury and Florez, Omar and Park, Serim and McWilliams, Brian and Han, Jiawei and El-Kishky, Ahmed},\n  journal={arXiv preprint arXiv:2209.07562},\n  year={2022}\n}\n\nhttps://towardsdatascience.com/pre-processing-should-extract-context-specific-features-4d01f6669a7e\n\ntokenization:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils.py\nhttps://github.com/google-research/bert/blob/master/tokenization.py\nnone the wiser on how the special tokens handels #, im guessing it doesn't extract the semantic meaning.\n\nhttps://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\nhttps://datasciencetoday.net/index.php/en-us/nlp/211-paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained\n\nhttps://lifewithdata.com/2023/05/27/transformermixin-in-scikit-learn/","metadata":{}}]}